{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-08T07:01:35.327395Z",
     "start_time": "2024-05-08T07:01:35.317567Z"
    }
   },
   "source": [
    "import matplotlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shutil\n",
    "import csv\n",
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "from datetime import timedelta\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "\n",
    "matplotlib.rcParams['font.sans-serif'] = ['SimHei']\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "base_dir = r'D:\\Data\\workspace\\python\\projects\\CNDataAuditOutput\\sichuan'\n",
    "dataset_dir = rf'{base_dir}\\datasets'\n",
    "empties_dir = rf'{base_dir}\\empty_datasets'\n",
    "integrity_issues_dir = rf'{base_dir}\\structural_issues_datasets'\n",
    "catalog_path = rf'{base_dir}\\dataset_catalog.json'\n",
    "null_values = ['无', '未知', '/', '-', '', ' ', '&nbsp;', 'null', 'NULL', 'N/A', ]\n",
    "metrics = ['integrity', 'uniqueness', 'completeness', 'timeliness']\n",
    "frequency_to_days = {'实时': 0, '每天': 1, '每周': 7, '每月': 30,\n",
    "                     '每季度': 90, '每半年': 183, '每年': 365}\n",
    "\n",
    "def read_csv(dataset_name, directory=dataset_dir):\n",
    "    return pd.read_csv(dataset_file_path(dataset_name, directory),\n",
    "                       encoding='gbk', na_values=null_values,\n",
    "                       encoding_errors='ignore')\n",
    "\n",
    "def dataset_file_path(filename: str, directory=dataset_dir):\n",
    "    if not filename.endswith('.csv'):\n",
    "        filename += '.csv'\n",
    "    return os.path.join(directory, filename)\n",
    "\n",
    "def exists(dataset_name, directory=dataset_dir):\n",
    "    return os.path.exists(dataset_file_path(dataset_name, directory))\n",
    "\n",
    "def dataset_files(directory=dataset_dir):\n",
    "    return (f for f in os.listdir(directory) if f.endswith('.csv'))\n",
    "\n",
    "def evaluate_metric(df, metric_function, *args):\n",
    "    result = df.copy()\n",
    "    result[metric_function.__name__] = result.apply(metric_function, axis=1, args=args)\n",
    "    return result\n",
    "\n",
    "def evaluate_metric_parallel(df, metric_function):\n",
    "    result = df.copy()\n",
    "    result[metric_function.__name__] = Parallel(n_jobs=-1)(\n",
    "        delayed(metric_function)(name) for name in result['name'])\n",
    "    return result\n",
    "\n",
    "def pipeline(data, *funcs):\n",
    "    result = data.copy()\n",
    "    for func in funcs:\n",
    "        result = func(result)\n",
    "    return result\n"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "catalog_dtype = {'name': str, 'id': str, 'URL': str, 'owner': str, 'category': 'category',\n",
    "                 'published': 'datetime64[ns]', 'updated': 'datetime64[ns]',\n",
    "                 'frequency': 'category', 'sample_data': object}\n",
    "\n",
    "catalog_df = pd.read_json(catalog_path, dtype=catalog_dtype)\n",
    "print(catalog_df.info())\n",
    "print(catalog_df['category'].value_counts())\n",
    "print(catalog_df['frequency'].value_counts())\n",
    "print(catalog_df['owner'].value_counts())"
   ],
   "id": "8a0cce08c70abe1e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def detect_conflicts(df):\n",
    "    dataset_names_from_files = [os.path.splitext(f)[0] for f in dataset_files()]\n",
    "    dataset_names_from_catalog = df['name'].tolist()\n",
    "    datasets_not_in_catalog = set(dataset_names_from_files) - set(dataset_names_from_catalog)\n",
    "    datasets_in_catalog_not_found = set(dataset_names_from_catalog) - set(dataset_names_from_files)\n",
    "    \n",
    "    print('Datasets not in catalog:', len(datasets_not_in_catalog))\n",
    "    print('Datasets in catalog but not found in files:', len(datasets_in_catalog_not_found))\n",
    "    return datasets_not_in_catalog, datasets_in_catalog_not_found\n",
    "\n",
    "def remove_files(names):\n",
    "    count = 0\n",
    "    for dataset_name in names:\n",
    "      if exists(dataset_name):\n",
    "          os.remove(dataset_file_path(dataset_name))\n",
    "          count += 1\n",
    "      else:\n",
    "          print(f\"File not found: {dataset_file_path(dataset_name)}\")\n",
    "    print(count)\n",
    "\n",
    "def remove_records(names, df):\n",
    "    result = df[~df['name'].isin(names)]\n",
    "    sample_data = result['sample_data'].copy()\n",
    "    result = result.astype(str)\n",
    "    result['sample_data'] = sample_data\n",
    "    return result\n",
    "\n",
    "def remove_conflicted_datasets(df):\n",
    "    datasets_not_in_catalog, datasets_in_catalog_not_found = detect_conflicts(df)\n",
    "    remove_files(datasets_not_in_catalog)\n",
    "    return remove_records(datasets_in_catalog_not_found, df)\n",
    "\n",
    "def remove_duplicated_datasets_by_name(df):\n",
    "    duplicates = df[df.duplicated('name', keep=False)]\n",
    "    print(\"Duplicate records based on 'name':\",\n",
    "          len(duplicates), duplicates['name'].sort_values())\n",
    "    remove_files(set(duplicates['name'].tolist()))\n",
    "    return remove_records(set(duplicates['name'].tolist()), df)\n",
    "\n",
    "\n",
    "# filtered = pipeline(catalog_df, remove_conflicted_datasets,\n",
    "#                     remove_duplicated_datasets_by_name)\n",
    "# filtered.to_json('updated_catalog.json', orient='records', force_ascii=False)"
   ],
   "id": "dbc5f03a4dc04738",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def move_empty_datasets(df):\n",
    "    os.makedirs(empties_dir, exist_ok=True)\n",
    "    is_empty_sample = df['sample_data'].apply(lambda x: x == {'null': 'null'})\n",
    "    \n",
    "    for index, row in df[is_empty_sample].iterrows():\n",
    "        file_name = f'{row[\"name\"]}.csv'\n",
    "        if exists(file_name):\n",
    "            source_path = dataset_file_path(file_name)\n",
    "            destination_path = dataset_file_path(file_name)\n",
    "            shutil.move(source_path, destination_path)\n",
    "            print(f'Moved \"{file_name}\" to {empties_dir}')\n",
    "\n",
    "move_empty_datasets(catalog_df)"
   ],
   "id": "8e6a7dc036a79e40",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def evaluate_csv_structural_integrity(df):\n",
    "    return evaluate_metric_parallel(df, integrity)\n",
    "\n",
    "def integrity(dataset_name):\n",
    "    if exists(dataset_name, dataset_dir):\n",
    "        return 1.0\n",
    "    if exists(dataset_name, empties_dir):\n",
    "        return np.nan\n",
    "\n",
    "    csv_path = dataset_file_path(dataset_name, integrity_issues_dir)\n",
    "    return count_good_rows(csv_path) / count_total_rows(csv_path)\n",
    "\n",
    "def count_total_rows(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='gbk', errors='ignore') as file:\n",
    "            return sum(1 for _ in csv.reader(file))\n",
    "    except IOError as e:\n",
    "        print(f'Error reading file {file_path}: {e}')\n",
    "        return 0\n",
    "\n",
    "def count_good_rows(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='gbk', errors='ignore') as file:\n",
    "            reader = csv.reader(file)\n",
    "            expected_columns = len(next(reader))\n",
    "            return sum(1 for row in reader if len(row) == expected_columns)\n",
    "    except IOError as e:\n",
    "        print(f'Error reading file {file_path}: {e}')\n",
    "        return 0\n",
    "\n",
    "def move_structural_issues(destination_dir):\n",
    "    os.makedirs(destination_dir, exist_ok=True)\n",
    "    for file in dataset_files():\n",
    "        if integrity(dataset_file_path(file)) != 1.0:\n",
    "            shutil.move(dataset_file_path(file), dataset_file_path(file, directory=destination_dir))\n",
    "            print(f\"Moved '{dataset_file_path(file)}' to '{dataset_file_path(file, directory=destination_dir)}'\")\n",
    "\n",
    "\n",
    "move_structural_issues(integrity_issues_dir)\n",
    "temp = evaluate_csv_structural_integrity(catalog_df)\n",
    "temp['integrity'].value_counts(dropna=False).sort_index()\n"
   ],
   "id": "ac8398002229f24a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def evaluate_uniqueness(df):\n",
    "    return evaluate_metric_parallel(df, uniqueness)\n",
    "\n",
    "def uniqueness(dataset_name):\n",
    "    if not exists(dataset_name, dataset_dir):\n",
    "        return np.nan\n",
    "    return 0 if read_csv(dataset_name).duplicated().any() else 1\n"
   ],
   "id": "7e97b56cef2f03a9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def evaluate_completeness(df):\n",
    "    return evaluate_metric_parallel(df, completeness)\n",
    "\n",
    "def completeness(dataset_name):\n",
    "    def _completeness(dataset_df):\n",
    "        return (dataset_df.size - dataset_df.isna().sum().sum()) / dataset_df.size\n",
    "    \n",
    "    return _completeness(read_csv(dataset_name)) if exists(dataset_name) else np.nan\n",
    "\n",
    "\n",
    "temp = evaluate_completeness(catalog_df)\n",
    "# temp['completeness'].hist()\n",
    "temp['completeness'].value_counts(dropna=False).sort_index()"
   ],
   "id": "806caf53f28b205a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def evaluate_timeliness(df):\n",
    "    downloaded = pd.Timestamp('2024-04-30 20:00:00')\n",
    "    return evaluate_metric(df, timeliness, downloaded, frequency_to_days)\n",
    "\n",
    "def timeliness(dataset, now, days_map):\n",
    "    days = days_map.get(dataset['frequency'], None)\n",
    "    if days is None:\n",
    "        return np.nan\n",
    "    return 1.0 if now - dataset['updated'] <= timedelta(days=days) else 0.0\n",
    "\n",
    "def timeliness_freq_distribution(df):\n",
    "    timely_df = df[df['timeliness'] == 1.0]\n",
    "    untimely_df = df[df['timeliness'] == 0.0]\n",
    "    return timely_df['frequency'].value_counts(), untimely_df['frequency'].value_counts()\n",
    "\n",
    "def print_timeliness_freq_distribution(df):\n",
    "    timely_freq_counts, untimely_freq_counts = timeliness_freq_distribution(df)\n",
    "    print('Frequency distribution for timely updates:')\n",
    "    print(timely_freq_counts)\n",
    "    print('Frequency distribution for untimely updates:')\n",
    "    print(untimely_freq_counts)\n",
    "\n",
    "def visualize_timeliness_distribution(df, save_path=None):\n",
    "    timely_freq_counts, untimely_freq_counts = timeliness_freq_distribution(df)\n",
    "    _timely_freq_counts = timely_freq_counts[timely_freq_counts > 0]\n",
    "    _untimely_freq_counts = untimely_freq_counts[untimely_freq_counts > 0]\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 9))\n",
    "    axes[0].pie(_timely_freq_counts, labels=_timely_freq_counts.index,\n",
    "                autopct=autopct_format(_timely_freq_counts), startangle=140)\n",
    "    axes[1].pie(_untimely_freq_counts, labels=_untimely_freq_counts.index,\n",
    "                autopct=autopct_format(_untimely_freq_counts), startangle=140)\n",
    "    axes[0].set_title('及时更新的数据集更新频率分布')\n",
    "    axes[1].set_title('未及时更新的数据集更新频率分布')\n",
    "    fig.text(0.5, 0.01, '评估时间：2024-04-30 20:00:00', ha='center', va='bottom', fontsize=10)\n",
    "    fig.tight_layout()\n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def autopct_format(values):\n",
    "    def my_format(pct):\n",
    "        val = int(round(pct * sum(values) / 100.0))\n",
    "        return '{v:d} ({p:.2f}%)'.format(v=val, p=pct) if pct > 0 else ''\n",
    "    return my_format\n",
    "\n",
    "temp = evaluate_timeliness(catalog_df)\n",
    "print_timeliness_freq_distribution(temp)\n",
    "# visualize_timeliness_distribution(save_path='timeliness_distribution.png')\n",
    "temp['timeliness'].value_counts(dropna=False)"
   ],
   "id": "a4797e355ca7ce02",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def evaluate_data_quality(df):\n",
    "    return pipeline(df, evaluate_csv_structural_integrity,\n",
    "                    evaluate_uniqueness, evaluate_completeness,\n",
    "                    evaluate_timeliness)\n",
    "\n",
    "def view(df):\n",
    "    return df[['name', 'owner', 'category', 'integrity', 'uniqueness', 'completeness', 'timeliness', ]]\n",
    "\n",
    "evaluated_df = evaluate_data_quality(catalog_df)"
   ],
   "id": "1d8a81df510c577d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def analyze_data_quality(df):\n",
    "    data_quality_df = view(df)\n",
    "\n",
    "    print(\"Basic Statistics:\")\n",
    "    print(data_quality_df[metrics].describe())\n",
    "\n",
    "    print(\"\\nMissing Values Count:\")\n",
    "    print(data_quality_df[metrics].isnull().sum())\n",
    "\n",
    "    print(\"\\nValue Counts:\")\n",
    "    for metric in metrics:\n",
    "        print(data_quality_df[metric].value_counts(dropna=False))\n",
    "\n",
    "    print(\"\\nProportion of Categories:\")\n",
    "    for metric in metrics:\n",
    "        print(data_quality_df[metric].value_counts(dropna=False) / len(data_quality_df))\n",
    "\n",
    "\n",
    "analyze_data_quality(evaluated_df)"
   ],
   "id": "d08d3a00ab31a818",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def visualize_data_quality(df, save_path=None):\n",
    "    bins = (0, 0.25, 0.5, 0.75, 1)\n",
    "    bin_labels = ('0-0.25', '0.25-0.5', '0.5-0.75', '0.75-1', 'NaN')\n",
    "\n",
    "    for metric in metrics:\n",
    "        _, ax = plt.subplots(figsize=(9, 6), dpi=240)\n",
    "        if metric in ['integrity', 'completeness']:\n",
    "            plot_category_chart(ax, metric, df.copy(), bins, bin_labels)\n",
    "        else:\n",
    "            plot_category_chart(ax, metric, df.copy())\n",
    "        if save_path is not None:\n",
    "            plt.savefig(f'{save_path}_{metric}.png')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_category_chart(ax, column, df, bins=None, bin_labels=None):\n",
    "    if bins and bin_labels:\n",
    "        df['_Category'] = pd.cut(df[column].dropna(), bins=bins, labels=bin_labels[:-1],\n",
    "                                include_lowest=True)\n",
    "        df['_Category'] = df['_Category'].astype(str)\n",
    "        df.loc[df[column].isna(), '_Category'] = 'NaN'\n",
    "    else:\n",
    "        df['_Category'] = df[column].map(label_dict_of(column)).fillna('NaN')\n",
    "\n",
    "    sns.countplot(x='_Category', data=df, ax=ax, palette='viridis', \n",
    "                  order=bin_labels if bins else sorted(df['_Category'].unique()))\n",
    "    ax.set_title(f'Bar Chart of {column}')\n",
    "    ax.set_xlabel('Categories')\n",
    "    ax.set_ylabel('Count')\n",
    "\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                    ha='center', va='center', xytext=(0, 5), textcoords='offset points')\n",
    "\n",
    "def label_dict_of(column):\n",
    "    return defaultdict(lambda: '无法判断', {\n",
    "        'integrity': {1.0: '正常', 0.0: '格式问题'},\n",
    "        'completeness': {1.0: '完全', 0.0: '不完全'},\n",
    "        'uniqueness': {1.0: '唯一', 0.0: '不唯一'},\n",
    "        'timeliness': {1.0: '及时', 0.0: '不及时'}\n",
    "    }.get(column, {}))\n",
    "\n",
    "\n",
    "visualize_data_quality(evaluated_df, 'output')"
   ],
   "id": "68f8ca0b0b39e28b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7c50809e3a0f3b49"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
