{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-04T12:01:31.427942Z",
     "start_time": "2024-05-04T12:01:31.414977Z"
    }
   },
   "source": [
    "import matplotlib\n",
    "import seaborn as sns\n",
    "from functools import partial\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import csv\n",
    "import os\n",
    "from enum import Enum, auto\n",
    "\n",
    "class DataQualityIssue(Enum):\n",
    "    STRUCTURAL_INTEGRITY_ISSUES = auto()\n",
    "    UN_TIMELY = auto()\n",
    "    EMPTY = auto()\n",
    "    INCONSISTENT = auto()\n",
    "    DUPLICATE = auto()\n",
    "    FORMAT_ERROR = auto()\n",
    "\n",
    "\n",
    "class Timeliness(Enum):\n",
    "    TIMELY = '及时'\n",
    "    UNTIMELY = '不及时'\n",
    "    UNDETERMINED = '无法判断'\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.value\n",
    "\n",
    "matplotlib.rcParams['font.sans-serif'] = ['SimHei']\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "dataset_dir = r'D:\\Data\\workspace\\python\\projects\\CNDataAuditOutput\\sichuan\\datasets'\n",
    "empties_dir = r'D:\\Data\\workspace\\python\\projects\\CNDataAuditOutput\\sichuan\\empty_datasets'\n",
    "integrity_issues_dir = r'D:\\Data\\workspace\\python\\projects\\CNDataAuditOutput\\sichuan\\structural_issues_datasets'\n",
    "catalog_path = r'D:\\Data\\workspace\\python\\projects\\CNDataAuditOutput\\sichuan\\dataset_catalog.json'\n",
    "catalog_dtype = {'name': str, 'id': str, 'URL': str, 'owner': str, 'category': 'category',\n",
    "                 'published': 'datetime64[ns]', 'updated': 'datetime64[ns]',\n",
    "                 'frequency': 'category', 'sample_data': object}\n",
    "null_values = ['无', '未知', '/', '-', '', ' ', '&nbsp;', 'null', 'NULL', 'N/A', ]\n",
    "\n",
    "\n",
    "def read_csv(dataset_name, directory=dataset_dir):\n",
    "    return pd.read_csv(dataset_file_path(dataset_name, directory),\n",
    "                       encoding='gbk', na_values=null_values,\n",
    "                       encoding_errors='ignore')\n",
    "\n",
    "def dataset_files(directory=dataset_dir):\n",
    "    return (f for f in os.listdir(directory) if f.endswith('.csv'))\n",
    "\n",
    "def dataset_file_path(filename: str, directory=dataset_dir):\n",
    "    if not filename.endswith('.csv'):\n",
    "        filename += '.csv'\n",
    "    return os.path.join(directory, filename)\n",
    "\n",
    "def dataset_completeness(dataset_df):\n",
    "    return ((dataset_df.size - dataset_df.isna().sum().sum()) / dataset_df.size) * 100\n",
    "\n",
    "def exists(dataset_name, directory=dataset_dir):\n",
    "    return os.path.exists(dataset_file_path(dataset_name, directory))\n",
    "\n",
    "def view(catalog_df):\n",
    "    return catalog_df[['name', 'owner', 'category', 'completeness', 'timeliness', ]]"
   ],
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T12:04:59.466794Z",
     "start_time": "2024-05-04T12:04:59.346582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "catalog_df = pd.read_json(catalog_path, dtype=catalog_dtype)\n",
    "print(catalog_df.info())\n",
    "print(catalog_df['category'].value_counts())\n",
    "print(catalog_df['frequency'].value_counts())"
   ],
   "id": "8a0cce08c70abe1e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3777 entries, 0 to 3776\n",
      "Data columns (total 9 columns):\n",
      " #   Column       Non-Null Count  Dtype         \n",
      "---  ------       --------------  -----         \n",
      " 0   name         3777 non-null   object        \n",
      " 1   id           3777 non-null   object        \n",
      " 2   URL          3777 non-null   object        \n",
      " 3   owner        3777 non-null   object        \n",
      " 4   category     3777 non-null   category      \n",
      " 5   published    3777 non-null   datetime64[ns]\n",
      " 6   updated      3777 non-null   datetime64[ns]\n",
      " 7   frequency    3777 non-null   category      \n",
      " 8   sample_data  3776 non-null   object        \n",
      "dtypes: category(2), datetime64[ns](2), object(5)\n",
      "memory usage: 215.1+ KB\n",
      "None\n",
      "category\n",
      "社保就业         946\n",
      "医疗卫生         463\n",
      "市场监管         360\n",
      "生活服务         355\n",
      "教育文化         234\n",
      "生态环境         219\n",
      "工业农业         189\n",
      "公共安全         140\n",
      "信用服务         132\n",
      "财税金融         124\n",
      "交通运输         110\n",
      "城建住房         107\n",
      "能源资源          80\n",
      "社会救助          78\n",
      "机构团体          53\n",
      "商贸流通          48\n",
      "气象服务          38\n",
      "科技创新          31\n",
      "法律服务          24\n",
      "地理空间          23\n",
      "安全生产          20\n",
      "工业农业 财税金融      1\n",
      "机构团体 教育文化      1\n",
      "能源资源 交通运输      1\n",
      "Name: count, dtype: int64\n",
      "frequency\n",
      "不定期    1818\n",
      "每年      981\n",
      "每天      330\n",
      "实时      246\n",
      "每月      213\n",
      "每半年     117\n",
      "每季度      54\n",
      "每周       18\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def compare_datasets(catalog_df):\n",
    "    dataset_names_from_files = [os.path.splitext(f)[0] for f in dataset_files()]\n",
    "    dataset_names_from_catalog = catalog_df['name'].tolist()\n",
    "    datasets_not_in_catalog = set(dataset_names_from_files) - set(dataset_names_from_catalog)\n",
    "    datasets_in_catalog_not_found = set(dataset_names_from_catalog) - set(dataset_names_from_files)\n",
    "    return datasets_not_in_catalog, datasets_in_catalog_not_found\n",
    "\n",
    "\n",
    "datasets_not_in_catalog, datasets_in_catalog_not_found = compare_datasets(catalog_df)\n",
    "print(\"Datasets not in catalog:\", len(datasets_not_in_catalog))\n",
    "print(\"Datasets in catalog but not found in files:\", len(datasets_in_catalog_not_found))\n",
    "\n",
    "\n",
    "def remove_files(names):\n",
    "    count = 0\n",
    "    for dataset_name in names:\n",
    "      if exists(dataset_name):\n",
    "          os.remove(dataset_file_path(dataset_name))\n",
    "          count += 1\n",
    "      else:\n",
    "          print(f\"File not found: {dataset_file_path(dataset_name)}\")\n",
    "    print(count)\n",
    "\n",
    "def remove_records(names):\n",
    "    filtered_df = catalog_df[~catalog_df['name'].isin(names)]\n",
    "    str_filtered_df = filtered_df.astype(str)\n",
    "    # last column as type dict\n",
    "    str_filtered_df['sample_data'] = filtered_df['sample_data']\n",
    "    print(len(str_filtered_df))\n",
    "    return str_filtered_df\n",
    "\n",
    "\n",
    "# remove_files(datasets_not_in_catalog)\n",
    "# filtered = remove_records(set(duplicates['name'].tolist()))\n",
    "# filtered.to_json('updated_catalog.json', orient='records', force_ascii=False)"
   ],
   "id": "dbc5f03a4dc04738"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def remove_duplicated_datasets_by_name():\n",
    "    duplicates = catalog_df[catalog_df.duplicated('name', keep=False)]\n",
    "    print(\"Duplicate records based on 'name':\")\n",
    "    print(len(duplicates), duplicates['name'].sort_values())\n",
    "    remove_files(set(duplicates['name'].tolist()))\n",
    "\n",
    "remove_duplicated_datasets_by_name()"
   ],
   "id": "195bc604fd28d640"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def move_empty_datasets():\n",
    "    os.makedirs(empties_dir, exist_ok=True)\n",
    "    is_empty_sample = catalog_df['sample_data'].apply(lambda x: x == {'null': 'null'})\n",
    "    \n",
    "    for index, row in catalog_df[is_empty_sample].iterrows():\n",
    "        file_name = f\"{row['name']}.csv\"\n",
    "        source_path = dataset_file_path(file_name)\n",
    "        destination_path = dataset_file_path(file_name)\n",
    "        shutil.move(source_path, destination_path)\n",
    "        print(f\"Moved '{file_name}' to {empties_dir}\")"
   ],
   "id": "8e6a7dc036a79e40",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T07:22:54.100982Z",
     "start_time": "2024-05-04T07:22:49.981694Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_csv_structural_integrity():\n",
    "    integrity_issues = {}\n",
    "    for file in dataset_files():\n",
    "        file_name = os.path.splitext(file)[0]\n",
    "        integrity_issues[file_name] = detect_csv_structural_issues(dataset_file_path(file_name))\n",
    "    return pd.DataFrame(list(integrity_issues.items()), columns=['文件名', '首个异常行标'])\n",
    "\n",
    "\n",
    "def detect_csv_structural_issues(file_path):\n",
    "    with open(file_path, encoding='gbk', errors='ignore') as file:\n",
    "        reader = csv.reader(file)\n",
    "        expected_columns = len(next(reader))\n",
    "        for row_index, row in enumerate(reader, 1):\n",
    "            if len(row) != expected_columns:\n",
    "                return row_index\n",
    "    return -1\n",
    "\n",
    "def move_structural_issues(destination_dir):\n",
    "    os.makedirs(destination_dir, exist_ok=True)\n",
    "    for file in dataset_files():\n",
    "        if detect_csv_structural_issues(dataset_file_path(file)) != -1:\n",
    "            shutil.move(dataset_file_path(file), dataset_file_path(file, directory=destination_dir))\n",
    "            print(f\"Moved '{dataset_file_path(file)}' to '{dataset_file_path(file, directory=destination_dir)}'\")\n",
    "\n",
    "\n",
    "integrity_df = evaluate_csv_structural_integrity()\n",
    "# move_structural_issues(anomalies_dir)\n",
    "\n",
    "print(integrity_df['首个异常行标'].value_counts())\n",
    "integrity_df[integrity_df['首个异常行标'] != -1]"
   ],
   "id": "ac8398002229f24a",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T12:04:51.960130Z",
     "start_time": "2024-05-04T12:04:51.955143Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_completeness(catalog_df):\n",
    "    catalog_df['completeness'] = compute_completeness(catalog_df['name'])\n",
    "\n",
    "def compute_completeness(dataset_names):\n",
    "    def _completeness(dataset_name):\n",
    "        return (dataset_completeness(read_csv(dataset_name))\n",
    "                if exists(dataset_name) else None)\n",
    "    \n",
    "    return [_completeness(dataset_name) for dataset_name in dataset_names]\n",
    "\n",
    "# catalog_df['completeness'].hist()\n",
    "# catalog_df['completeness'].value_counts(dropna=False)"
   ],
   "id": "806caf53f28b205a",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "def evaluate_timeliness(catalog_df):\n",
    "    def is_timely(dataset, now):\n",
    "        frequency_to_days = {\n",
    "            '实时': 0, '每天': 1, '每周': 7, '每月': 30,\n",
    "            '每季度': 90, '每半年': 183, '每年': 365\n",
    "        }\n",
    "        max_days = frequency_to_days.get(dataset['frequency'], None)\n",
    "        if max_days is None:\n",
    "            return Timeliness.UNDETERMINED\n",
    "        return (Timeliness.TIMELY\n",
    "                if now - dataset['updated'] <= timedelta(days=max_days)\n",
    "                else Timeliness.UNTIMELY)\n",
    "\n",
    "\n",
    "    downloaded = pd.Timestamp('2024-04-30 20:00:00')\n",
    "    is_timely = partial(is_timely, now=downloaded)\n",
    "    catalog_df['timeliness'] = catalog_df.apply(is_timely, axis=1)\n",
    "\n",
    "\n",
    "def print_timeliness_freq_distribution():\n",
    "    print('Frequency distribution for timely updates:')\n",
    "    print(timely_freq_counts)\n",
    "    print('Frequency distribution for untimely updates:')\n",
    "    print(untimely_freq_counts)\n",
    "\n",
    "\n",
    "def visualize_timeliness_distribution(save_path=None):\n",
    "    def autopct_format(values):\n",
    "        def my_format(pct):\n",
    "            total = sum(values)\n",
    "            val = int(round(pct*total/100.0))\n",
    "            return '{v:d} ({p:.2f}%)'.format(v=val, p=pct) if pct > 0 else ''\n",
    "        return my_format\n",
    "\n",
    "    _timely_freq_counts = timely_freq_counts[timely_freq_counts > 0]\n",
    "    _untimely_freq_counts = untimely_freq_counts[untimely_freq_counts > 0]\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 9))\n",
    "    axes[0].pie(_timely_freq_counts, labels=_timely_freq_counts.index,\n",
    "                autopct=autopct_format(_timely_freq_counts), startangle=140)\n",
    "    axes[1].pie(_untimely_freq_counts, labels=_untimely_freq_counts.index,\n",
    "                autopct=autopct_format(_untimely_freq_counts), startangle=140)\n",
    "    axes[0].set_title('及时更新的数据集更新频率分布')\n",
    "    axes[1].set_title('未及时更新的数据集更新频率分布')\n",
    "    fig.text(0.5, 0.01, '评估时间：2024-04-30 20:00:00', ha='center', va='bottom', fontsize=10)\n",
    "    fig.tight_layout()\n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print('Unique frequencies in the dataset:', catalog_df['frequency'].unique())\n",
    "\n",
    "evaluate_timeliness(catalog_df)\n",
    "\n",
    "timely_df = catalog_df[catalog_df['timeliness'] == Timeliness.TIMELY]\n",
    "untimely_df = catalog_df[catalog_df['timeliness'] == Timeliness.UNTIMELY]\n",
    "timely_freq_counts = timely_df['frequency'].value_counts()\n",
    "untimely_freq_counts = untimely_df['frequency'].value_counts()\n",
    "\n",
    "print_timeliness_freq_distribution()\n",
    "# visualize_timeliness_distribution(save_path='timeliness_distribution.png')\n",
    "catalog_df['timeliness'].value_counts(dropna=False)"
   ],
   "id": "a4797e355ca7ce02"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T12:05:27.483711Z",
     "start_time": "2024-05-04T12:05:16.162005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_data_quality(catalog_df):\n",
    "    evaluate_completeness(catalog_df)\n",
    "    evaluate_timeliness(catalog_df)\n",
    "\n",
    "evaluate_data_quality(catalog_df)"
   ],
   "id": "1d8a81df510c577d",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T12:05:44.001362Z",
     "start_time": "2024-05-04T12:05:43.991389Z"
    }
   },
   "cell_type": "code",
   "source": [
    "catalog_view = view(catalog_df)\n",
    "catalog_view.sample(5)"
   ],
   "id": "aa9d079fa3566d17",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                               name        owner category timeliness  \\\n",
       "384   乐山市_井研县_国家税务总局井研县税务局印花税税款申报信息          井研县     财税金融         及时   \n",
       "934           南充市_阆中市_阆中市脱贫人口外出务工信息          南充市     社保就业         及时   \n",
       "1039                 资阳市_住建局_施工劳务企业  资阳市住房和城乡建设局     城建住房       无法判断   \n",
       "3277          泸州市_泸县_城乡居民一类门诊特殊疾病明细    泸县医疗保险管理局     医疗卫生       无法判断   \n",
       "431      乐山市_金口河区_金口河区养老待遇领取资格认证统计表         金口河区     生活服务         及时   \n",
       "\n",
       "      completeness  \n",
       "384            NaN  \n",
       "934            NaN  \n",
       "1039    100.000000  \n",
       "3277           NaN  \n",
       "431      95.214286  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>owner</th>\n",
       "      <th>category</th>\n",
       "      <th>timeliness</th>\n",
       "      <th>completeness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>乐山市_井研县_国家税务总局井研县税务局印花税税款申报信息</td>\n",
       "      <td>井研县</td>\n",
       "      <td>财税金融</td>\n",
       "      <td>及时</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>南充市_阆中市_阆中市脱贫人口外出务工信息</td>\n",
       "      <td>南充市</td>\n",
       "      <td>社保就业</td>\n",
       "      <td>及时</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1039</th>\n",
       "      <td>资阳市_住建局_施工劳务企业</td>\n",
       "      <td>资阳市住房和城乡建设局</td>\n",
       "      <td>城建住房</td>\n",
       "      <td>无法判断</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3277</th>\n",
       "      <td>泸州市_泸县_城乡居民一类门诊特殊疾病明细</td>\n",
       "      <td>泸县医疗保险管理局</td>\n",
       "      <td>医疗卫生</td>\n",
       "      <td>无法判断</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>乐山市_金口河区_金口河区养老待遇领取资格认证统计表</td>\n",
       "      <td>金口河区</td>\n",
       "      <td>生活服务</td>\n",
       "      <td>及时</td>\n",
       "      <td>95.214286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T12:08:54.079954Z",
     "start_time": "2024-05-04T12:08:54.054023Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "20e66ade4f0f7bf0",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported type for timedelta days component: NoneType",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[51], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtimedelta\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mTypeError\u001B[0m: unsupported type for timedelta days component: NoneType"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d08d3a00ab31a818"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
